{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6aiV-SyxYb-"
      },
      "source": [
        "<h3><strong style=\"color:purple;\">Author Name: Anirban Bose</strong></h3>\n",
        "<h3><strong style=\"color:purple;\">App Name: Personalized AI Tutor for Advanced NLP</strong></h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaptDs3t9HmF"
      },
      "source": [
        "## 1: Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "m_I1tJkA843m"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio transformers datasets sentence-transformers langchain faiss-cpu langchain-community trafilatura nltk langchain-openai openai nltk --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m2n_H-X6gQx",
        "outputId": "2a662d39-3d68-40ec-c964-0b4042ccd842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "AwVYLs309D8P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ast\n",
        "from collections import defaultdict\n",
        "import gradio as gr\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification\n",
        ")\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import trafilatura\n",
        "from openai import OpenAI\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "nBSKQimiDuv9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata  # only works if `userdata` is available\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CHATGPT_MODEL = \"gpt-4\""
      ],
      "metadata": {
        "id": "4Mm1x4Jm4s7a"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjcCU3Su-NuL"
      },
      "source": [
        "## 2: Create Dataset based on Huggingface LLM course"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1: Fetch the valid urls"
      ],
      "metadata": {
        "id": "M2dkwbs_3T2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SajRiwWozeep",
        "outputId": "78a32816-13f5-4ea3-b78f-8fb897294f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 97 valid URLs\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def get_valid_llm_course_urls(base_url=\"https://huggingface.co/learn/llm-course\", max_chapters=30, max_pages=50):\n",
        "    valid_urls = []\n",
        "\n",
        "    for chapter in range(1, max_chapters + 1):\n",
        "        for page in range(1, max_pages + 1):\n",
        "            url = f\"{base_url}/chapter{chapter}/{page}\"\n",
        "            response = requests.head(url)\n",
        "            if response.status_code == 200:\n",
        "                valid_urls.append(url)\n",
        "            else:\n",
        "                # stop checking more pages for this chapter once a 404 is hit\n",
        "                break\n",
        "\n",
        "    return valid_urls\n",
        "\n",
        "# Run this to get the list of valid URLs\n",
        "valid_urls = get_valid_llm_course_urls()\n",
        "print(f\"Found {len(valid_urls)} valid URLs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2: Create section chunks and convert them into Langchain document objects"
      ],
      "metadata": {
        "id": "yP4Pb1vv3hiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvSo4pG88dhV",
        "outputId": "3b18b91f-fcc2-4323-a942-658a400c3a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping and chunking entire sections: 100%|██████████| 97/97 [00:31<00:00,  3.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total chunks (sections): 751\n",
            "📌 Sample Section:\n",
            "Title: Introduction - Hugging Face LLM Course\n",
            "Content: Models Datasets Spaces Community Docs Enterprise Pricing   Log In Sign Up LLM Course documentation Introduction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "section_chunks = []\n",
        "\n",
        "for url in tqdm(valid_urls, desc=\"Scraping and chunking entire sections\"):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Page title (as section group identifier)\n",
        "        page_title = soup.title.string if soup.title else \"Untitled\"\n",
        "\n",
        "        # Prepare for text accumulation\n",
        "        content = []\n",
        "        section_title = page_title\n",
        "\n",
        "        for tag in soup.find_all(['h1', 'h2', 'h3', 'p', 'li']):\n",
        "            tag_text = tag.get_text(strip=True)\n",
        "            if tag.name in ['h1', 'h2', 'h3'] and content:\n",
        "                full_text = \" \".join(sent_tokenize(\" \".join(content)))\n",
        "                section_chunks.append({\n",
        "                    \"title\": section_title,\n",
        "                    \"content\": full_text,\n",
        "                    \"url\": url\n",
        "                })\n",
        "                section_title = tag_text\n",
        "                content = []\n",
        "            elif tag.name in ['p', 'li']:\n",
        "                content.append(tag_text)\n",
        "\n",
        "        # Capture last chunk\n",
        "        if content:\n",
        "            full_text = \" \".join(sent_tokenize(\" \".join(content)))\n",
        "            section_chunks.append({\n",
        "                \"title\": section_title,\n",
        "                \"content\": full_text,\n",
        "                \"url\": url\n",
        "            })\n",
        "\n",
        "# 🧾 Preview example\n",
        "print(f\"✅ Total chunks (sections): {len(section_chunks)}\")\n",
        "print(\"📌 Sample Section:\")\n",
        "print(\"Title:\", section_chunks[0]['title'])\n",
        "print(\"Content:\", section_chunks[0]['content'][:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyialkZrBlFj",
        "outputId": "58093835-44b7-4c5f-f605-88ff1b0ab80b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Introduction - Hugging Face LLM Course',\n",
              " 'content': 'Models Datasets Spaces Community Docs Enterprise Pricing   Log In Sign Up LLM Course documentation Introduction',\n",
              " 'url': 'https://huggingface.co/learn/llm-course/chapter1/1'}"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ],
      "source": [
        "section_chunks[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "DPamHEUaBG5I"
      },
      "outputs": [],
      "source": [
        "# Convert your section_chunks into LangChain Document objects\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=chunk[\"content\"],\n",
        "        metadata={\"title\": chunk[\"title\"], \"source\": chunk[\"url\"]}\n",
        "    )\n",
        "    for chunk in section_chunks\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"✅ Total number of document objects: {len(documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Baxdc56X4L76",
        "outputId": "adb2ae74-8e5d-4dfd-d6c3-1e74ab1a2f6e"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Total number of document objects: 751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S-pyzhOCCCP"
      },
      "source": [
        "## 3: Setup Retrieval-Augmented QA with FAISS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvJLasX4CQF3"
      },
      "source": [
        "### 3.1: Create FAISS vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "MmtZ8WuUCOEt"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "vectordb = FAISS.from_documents(documents, embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt4qwQIIEyk5"
      },
      "source": [
        "### 3.2: Setup Retriever and RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "931-ph-kJ7o8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
        "from langchain.llms import HuggingFacePipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "zWk7WI1uN5Zr"
      },
      "outputs": [],
      "source": [
        "# Setup Custom Prompt template\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"You are an expert NLP tutor. Use the context below to answer the question accurately.\n",
        "If the answer is not in the context, say \"I don't know.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "y7BDJ1LJE2SY"
      },
      "outputs": [],
      "source": [
        "# Use ChatGPT\n",
        "llm = ChatOpenAI(model_name=CHATGPT_MODEL, temperature=0.3)\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectordb.as_retriever(),\n",
        "    chain_type=\"stuff\",\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt_template}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCvPqUBigUIw"
      },
      "source": [
        "### 3.3: RAG-style QA function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "PtDuYuZdIaIm"
      },
      "outputs": [],
      "source": [
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "2MRBdIxbG2CB"
      },
      "outputs": [],
      "source": [
        "def ask_question(user_query):\n",
        "    try:\n",
        "        result = qa_chain(user_query)\n",
        "        answer = result[\"result\"]\n",
        "\n",
        "        if answer.strip().lower() == \"i don't know.\":\n",
        "            sources = \"NA\"\n",
        "        else:\n",
        "            sources = \"\\n\".join(\n",
        "                f\"- {doc.metadata.get('title', 'Unknown')} ({doc.metadata.get('source', 'No URL')})\"\n",
        "                for doc in result[\"source_documents\"]\n",
        "            )\n",
        "\n",
        "        return f\"📌 **Answer:**\\n{answer}\\n\\n🔗 **Sources:**\\n{sources}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ **Error:**\\n```\\n{traceback.format_exc()}\\n```\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7M4Za23gcsV"
      },
      "source": [
        "## 4: Generate and Score MCQ Quizzes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpUTrHUghgXN"
      },
      "source": [
        "### 4.1: Create Topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "LfbAtvKekouO"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "bD4NfYq84GKA"
      },
      "outputs": [],
      "source": [
        "# Get all unique titles and build title-to-doc mapping\n",
        "title_to_docs = defaultdict(list)\n",
        "unique_titles = set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "KyA4FJ7H4OXu"
      },
      "outputs": [],
      "source": [
        "for doc in documents:\n",
        "    title = doc.metadata.get(\"title\")\n",
        "    if title:\n",
        "        title_to_docs[title].append(doc)\n",
        "        unique_titles.add(title)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "NWDDjkK6iDR0"
      },
      "outputs": [],
      "source": [
        "# Get the list of topics from the documents\n",
        "topics = sorted(list(unique_titles))\n",
        "topic_chunks = [topics[i:i+50] for i in range(0, len(topics), 50)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOc8XH_-iEtZ",
        "outputId": "13dc043c-f3b8-4f36-8153-79edfa8ca8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [04:06<00:00, 20.58s/it]\n"
          ]
        }
      ],
      "source": [
        "#Group similar titles into broader topics using GPT\n",
        "dropdown_topics_set = set()\n",
        "topic_to_titles = defaultdict(set)\n",
        "\n",
        "for chunk in tqdm(topic_chunks):\n",
        "    prompt = f\"\"\"\n",
        "You are given a list of topic strings from an NLP course. Group them under broader educational umbrella topics. This is for Personalized AI Tutor for Advanced NLP App.\n",
        "\n",
        "Ignore navigation items, UI labels, quiz strings, etc.\n",
        "\n",
        "Return a Python dictionary in the format:\n",
        "{{\"Transformers\": [\"Transformer Architectures\", \"Understanding\"], \"Tokenization\": [\"Text Tokenization\", \"Tokenizer\"]}}\n",
        "\n",
        "Topics:\n",
        "{chunk}\n",
        "\n",
        "Output:\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=CHATGPT_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that categorizes topics.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        parsed = ast.literal_eval(response.choices[0].message.content)\n",
        "\n",
        "        if isinstance(parsed, dict):\n",
        "            for broad_topic, titles in parsed.items():\n",
        "                dropdown_topics_set.add(broad_topic)\n",
        "                topic_to_titles[broad_topic].update(titles)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ GPT Error:\", e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "dMzm7KqJ5cWu"
      },
      "outputs": [],
      "source": [
        "# Map broad topics to documents using title_to_docs\n",
        "topics_docs = defaultdict(list)\n",
        "\n",
        "for broad_topic, title_set in topic_to_titles.items():\n",
        "    for title in title_set:\n",
        "        if title in title_to_docs:\n",
        "            topics_docs[broad_topic].extend(title_to_docs[title])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9sjyMTd5gim",
        "outputId": "8c78a584-29f9-4b34-f948-33b19cff8f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Topics found: ['All', 'APIs and Libraries', 'Additional Resources', 'Advanced Features', 'Advanced Topics', 'Algorithms', 'Alternative Evaluation', 'Annotation', 'Applications', 'Architecture', 'Attention Mechanisms', 'Audio Processing', 'Best Practices and Limitations', 'Chatbot Development', 'Code Formatting', 'Course Completion', 'Course Information', 'Course Introduction', 'Course Navigation', 'Course Navigation and Assistance', 'Course Navigation and Interaction', 'Course Progress', 'Custom Implementation', 'Data Acquisition', 'Data Handling', 'Data Management', 'Data Preparation', 'Dataset Handling', 'Dataset Management', 'Dataset Review', 'Debugging', 'Demo Creation', 'Deployment', 'Domain Adaptation', 'Encoding', 'Entity Handling', 'Evaluation', 'File Management', 'Framework Selection', 'Git Operations', 'Hosting and Integration', 'Image Processing', 'Importance of NLP', 'Input Management', 'Key Concepts', 'Language Model Learning', 'Language Models', 'Language Understanding Models', 'Large Language Models', 'Learning', 'Learning About NLP', 'LoRA', 'Machine Learning', 'Memory Management', 'Merging', 'Methods and Functions', 'Metrics', 'Metrics and Variables', 'Miscellaneous', 'Model Architecture', 'Model Development', 'Model Documentation', 'Model Evaluation', 'Model Implementation', 'Model Inputs', 'Model Loading and Integration', 'Model Management', 'Model Prediction', 'Model Training', 'Model Training and Testing', 'Model Tuning', 'Model Understanding and Optimization', 'Modeling', 'Modeling and Prediction', 'Monitoring and Optimization', 'Next Steps', 'Normalization', 'Performance and Evaluation', 'Pipeline Understanding', 'Practical Exercises', 'Problem Solving', 'Reinforcement Learning', 'Sequence Models', 'Service Providers', 'Setup and Sharing', 'Software Development', 'Software Installation', 'Software Usage', 'Specific Techniques', 'Summarization', 'Task Management', 'Text Encoding', 'Text Generation', 'Text Processing', 'Tokenization', 'Tokenization Types', 'Tokenizer Construction', 'Training', 'Training Interpretation', 'Training Loops', 'Transformers', 'Translation', 'Troubleshooting', 'Understanding NLP', 'Understanding and Design', 'User Interface', 'Vectorization', 'Web Interface', 'Working with Data']\n",
            "✅ Number of Topics found: 109\n"
          ]
        }
      ],
      "source": [
        "# Final list of dropdown topics\n",
        "dropdown_topics = [\"All\"] + sorted(dropdown_topics_set)\n",
        "print(\"✅ Topics found:\", dropdown_topics)\n",
        "print(\"✅ Number of Topics found:\", len(dropdown_topics))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3D4BCVSriIR"
      },
      "source": [
        "### 4.2: Functions for the MCQ quizzes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "CWtX1pK3rrx6"
      },
      "outputs": [],
      "source": [
        "# Quiz state variables\n",
        "quiz_state = []\n",
        "current_index = 0\n",
        "score = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "EbDt6qkkGKDz"
      },
      "outputs": [],
      "source": [
        "# Sample question generator (replace with LLM-generated content)\n",
        "def get_mcq_questions_for_topic(topic, n):\n",
        "    # Select relevant documents for the topic\n",
        "    if topic == \"All\":\n",
        "        relevant_docs = sum(topics_docs.values(), [])\n",
        "    else:\n",
        "        relevant_docs = topics_docs.get(topic, [])\n",
        "\n",
        "    if not relevant_docs:\n",
        "        return [{\n",
        "            \"question\": \"No content available for this topic.\",\n",
        "            \"options\": [\"N/A\", \"N/A\", \"N/A\", \"N/A\"],\n",
        "            \"correct\": 0\n",
        "        }]\n",
        "\n",
        "    # Concatenate top N documents for context (limit token size)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in relevant_docs)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful assistant that generates multiple-choice questions (MCQs) for NLP topics.\n",
        "\n",
        "Using the context below, generate {n} MCQs. Each question must:\n",
        "- Have 4 options (as a list of strings).\n",
        "- Indicate the correct option index (0 for A, 1 for B, etc.).\n",
        "- Be returned in valid Python list format.\n",
        "\n",
        "Context:\n",
        "\\\"\\\"\\\"\n",
        "{context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Return a Python list of dictionaries in the following format (and nothing else):\n",
        "\n",
        "[\n",
        "  {{\n",
        "    \"question\": \"Your question?\",\n",
        "    \"options\": [\"Option A\", \"Option B\", \"Option C\", \"Option D\"],\n",
        "    \"correct\": 2\n",
        "  }},\n",
        "  ...\n",
        "]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=CHATGPT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        raw_output = response.choices[0].message.content.strip()\n",
        "        # Safely parse GPT output\n",
        "        mcqs = ast.literal_eval(raw_output)\n",
        "        print(raw_output)\n",
        "        if isinstance(mcqs, list) and all(\"question\" in q and \"options\" in q and \"correct\" in q for q in mcqs):\n",
        "            return mcqs[:n]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"❌ LLM Generation Failed:\", e)\n",
        "\n",
        "    return [{\n",
        "        \"question\": \"Failed to generate questions. Please try again.\",\n",
        "        \"options\": [\"Retry\", \"Retry\", \"Retry\", \"Retry\"],\n",
        "        \"correct\": 0\n",
        "    }]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "nf_1oKUvOg7s"
      },
      "outputs": [],
      "source": [
        "# Format a question with labeled options\n",
        "def format_question(question_obj, index):\n",
        "    qtext = f\"**Q{index+1}. {question_obj['question']}**\"\n",
        "    options = [f\"{chr(65+i)}. {opt}\" for i, opt in enumerate(question_obj[\"options\"])]\n",
        "    return qtext, options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "6Ies9kIfOkFU"
      },
      "outputs": [],
      "source": [
        "# Start the quiz\n",
        "def start_quiz(topic, n):\n",
        "    global quiz_state, current_index, score\n",
        "    quiz_state = get_mcq_questions_for_topic(topic, int(n))\n",
        "    current_index = 0\n",
        "    score = 0\n",
        "    qtext, options = format_question(quiz_state[0], current_index)\n",
        "    return qtext, gr.Radio(choices=options, visible=True), \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit answer only\n",
        "def submit_answer(selected):\n",
        "    global score\n",
        "    if not selected:\n",
        "        return gr.update(), gr.update(), \"⚠️ Please select an answer.\", gr.update(), gr.update(), gr.update()\n",
        "\n",
        "    correct_index = quiz_state[current_index][\"correct\"]\n",
        "    correct_ans = f\"{chr(65+correct_index)}. {quiz_state[current_index]['options'][correct_index]}\"\n",
        "\n",
        "    if selected == correct_ans:\n",
        "        score += 1\n",
        "        result = \"✅ Correct!\"\n",
        "    else:\n",
        "        result = f\"❌ Incorrect. Correct answer: **{correct_ans}**\"\n",
        "\n",
        "    score_status = f\"**Score:** {score}/{current_index+1}\"\n",
        "    feedback_text = f\"{result}<br>{score_status}\"\n",
        "\n",
        "    return gr.update(visible=False), gr.update(visible=False), feedback_text, gr.update(visible=False), gr.update(visible=False), gr.update(visible=True)\n"
      ],
      "metadata": {
        "id": "KQPsYFqKnuug"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go to next question\n",
        "def next_question():\n",
        "    global current_index\n",
        "    current_index += 1\n",
        "\n",
        "    if current_index < len(quiz_state):\n",
        "        qtext, options = format_question(quiz_state[current_index], current_index)\n",
        "        return  gr.update(value=qtext, visible=True), gr.Radio(choices=options, value=None, visible=True), \"\", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "    else:\n",
        "        final_score = f\"### 🏁 Quiz Complete! Your Score: **{score}/{len(quiz_state)}**\"\n",
        "        return \"\", gr.update(visible=False), final_score, gr.update(visible=False), gr.update(visible=True), gr.update(visible=False)\n"
      ],
      "metadata": {
        "id": "7zsUQfoQnxVH"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart quiz\n",
        "def restart_quiz():\n",
        "    return \"\", gr.update(visible=False), \"\", gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)\n"
      ],
      "metadata": {
        "id": "aw5F1KuBn2zw"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHrmh7Lcgnnh"
      },
      "source": [
        "## 5: Build Conversational Tutor with LangChain Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "pqkqz8IPgivr"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8T91GkSguQ4"
      },
      "source": [
        "### 5.1: Setup retriever, memory, llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "VqHek76-gszy"
      },
      "outputs": [],
      "source": [
        "# Setup retriever\n",
        "conversational_retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "wVtixt9BYhl7"
      },
      "outputs": [],
      "source": [
        "# Setup memory\n",
        "conversational_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "80B9Exric-P-"
      },
      "outputs": [],
      "source": [
        "# Setup llm\n",
        "conversational_llm = ChatOpenAI(model_name=CHATGPT_MODEL, temperature=0.6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV2iGTyZg4Og"
      },
      "source": [
        "### 5.2: Set up LangChain QA agent with memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "SFQxJLR1gyi8"
      },
      "outputs": [],
      "source": [
        "conversational_qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=conversational_llm,\n",
        "    retriever=conversational_retriever,\n",
        "    memory=conversational_memory,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(user_message, history):\n",
        "    try:\n",
        "        # Try RAG-based QA first\n",
        "        result = conversational_qa_chain({\"question\": user_message})\n",
        "        answer = result[\"answer\"].strip()\n",
        "\n",
        "        # Fallback if RAG fails to answer\n",
        "        if answer.lower() in [\"i don't know.\", \"i don't know\", \"i cannot answer that.\"]:\n",
        "            fallback_answer = conversational_llm.predict(user_message)\n",
        "            answer = f\"(Fallback Answer)\\n{fallback_answer}\"\n",
        "\n",
        "        history.append((user_message, answer))\n",
        "        return history, history\n",
        "\n",
        "    except Exception as e:\n",
        "        history.append((user_message, f\"❌ Error: {str(e)}\"))\n",
        "        return history, history\n"
      ],
      "metadata": {
        "id": "lz24BBezyKyE"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "id": "BP5Mqk5makOY"
      },
      "outputs": [],
      "source": [
        "def clear_chat():\n",
        "    conversational_memory.clear()\n",
        "    return [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk8EM1FunbEi"
      },
      "source": [
        "## 6: Add DNLI-Style Reasoning Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs77Qob0nqYK"
      },
      "source": [
        "### 6.1: Functions for reasoning check"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_premise_from_docs(topic):\n",
        "    docs = topics_docs.get(topic, [])\n",
        "    if not docs:\n",
        "        return \"No content found.\"\n",
        "\n",
        "    # Filter out boilerplate or low-content text\n",
        "    meaningful_docs = [\n",
        "        doc.page_content.strip() for doc in docs\n",
        "        if len(doc.page_content.strip()) > 100 and\n",
        "        \"augmented documentation experience\" not in doc.page_content.lower()\n",
        "    ]\n",
        "\n",
        "    if not meaningful_docs:\n",
        "        return \"No meaningful content available after filtering.\"\n",
        "\n",
        "    # Join filtered documents into a single large context\n",
        "    full_context = \"\\n\\n\".join(meaningful_docs)\n",
        "\n",
        "    # Truncate if context is too long\n",
        "    truncated_context = full_context[:6000]  # approx ~2,000 tokens\n",
        "\n",
        "    # Prepare the summarization prompt\n",
        "    prompt = f\"\"\"\n",
        "You are an expert NLP assistant. Summarize the following technical content clearly and concisely within 30 sentences.\n",
        "\n",
        "Content:\n",
        "\\\"\\\"\\\"\n",
        "{truncated_context}\n",
        "\\\"\\\"\\\"\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=CHATGPT_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.4\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Error while summarizing: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "4USdeL7J4X5F"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "id": "okmbqExhrQlE"
      },
      "outputs": [],
      "source": [
        "def populate_premise(topic):\n",
        "        return get_premise_from_docs(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "4QXJPvignm5o"
      },
      "outputs": [],
      "source": [
        "def dnli_reasoning_check(premise, hypothesis):\n",
        "        prompt = f\"\"\"\n",
        "You're given a premise from a technical document and a user-generated hypothesis.\n",
        "\n",
        "Classify their relationship as one of the following:\n",
        "- entailment (hypothesis logically follows),\n",
        "- neutral (uncertain),\n",
        "- contradiction (hypothesis contradicts the premise).\n",
        "\n",
        "Respond with one word: \"entailment\", \"neutral\", or \"contradiction\".\n",
        "\n",
        "Premise: \"{premise}\"\n",
        "Hypothesis: \"{hypothesis}\"\n",
        "\"\"\"\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=CHATGPT_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.3\n",
        "            )\n",
        "            result = response.choices[0].message.content.strip().lower()\n",
        "            return f\"### 🔎 Result: **{result.capitalize()}**\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU568eTYlj_C"
      },
      "source": [
        "## 7: Build Gradio App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ZaoZ83vHG81f",
        "outputId": "ec67275f-76a3-4725-e283-0779a0f7722d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-243-4168299844.py:28: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(label=\"Your Tutor\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://04b0a403e1364bf584.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://04b0a403e1364bf584.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 243
        }
      ],
      "source": [
        "\n",
        "\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# 🧠 Personalized AI Tutor for Advanced NLP\")\n",
        "\n",
        "    with gr.Tab(\"📖 Ask a Question (RAG)\"):\n",
        "        q = gr.Textbox(label=\"Your Question\")\n",
        "        a = gr.Textbox(label=\"AI Answer + Context\")\n",
        "        gr.Button(\"Submit\").click(ask_question, inputs=q, outputs=a)\n",
        "\n",
        "    with gr.Tab(\"📚 Take a Quiz\"):\n",
        "        gr.Markdown(\"## 🧠 Personalized Quiz from Course Topics\")\n",
        "\n",
        "        topic = gr.Dropdown(choices=dropdown_topics, label=\"Select Topic\")\n",
        "        num_qs = gr.Slider(label=\"Number of Questions\", minimum=1, maximum=10, step=1, value=5)\n",
        "\n",
        "        start = gr.Button(\"Start Quiz\")\n",
        "        qtext = gr.Markdown()\n",
        "        options = gr.Radio(choices=[], label=\"Options\", visible=False)\n",
        "        feedback = gr.Markdown()\n",
        "        submit = gr.Button(\"Submit\", visible=False)\n",
        "        next_btn = gr.Button(\"Next\", visible=False)\n",
        "        restart = gr.Button(\"Restart\", visible=False)\n",
        "        start.click(start_quiz, inputs=[topic, num_qs], outputs=[qtext, options, feedback, submit, restart, next_btn])\n",
        "        submit.click(submit_answer, inputs=options, outputs=[qtext, options, feedback, submit, restart, next_btn])\n",
        "        next_btn.click(next_question, outputs=[qtext, options, feedback, submit, restart, next_btn])\n",
        "        restart.click(restart_quiz, outputs=[qtext, options, feedback, submit, restart, next_btn])\n",
        "\n",
        "    with gr.Tab(\"🗣️ Conversational Tutor\"):\n",
        "        chatbot = gr.Chatbot(label=\"Your Tutor\")\n",
        "        msg = gr.Textbox(label=\"Ask something\", placeholder=\"Start your conversation...\")\n",
        "        submit_btn = gr.Button(\"Submit\")\n",
        "        clear_btn = gr.Button(\"End Chat\")\n",
        "\n",
        "        state = gr.State([])\n",
        "\n",
        "        submit_btn.click(chat, inputs=[msg, state], outputs=[chatbot, state])\n",
        "        clear_btn.click(clear_chat, inputs=None, outputs=[chatbot, state])\n",
        "\n",
        "    with gr.Tab(\"🔍 Reasoning Check\"):\n",
        "        gr.Markdown(\"## 🔍 DNLI-Style Reasoning Check\")\n",
        "\n",
        "        topic_dropdown = gr.Dropdown(choices=dropdown_topics[1:], label=\"Select Topic\")  # Exclude \"All\"\n",
        "        premise_input = gr.Textbox(label=\"Premise\", lines=3, interactive=False)\n",
        "        hypothesis_input = gr.Textbox(label=\"Hypothesis\", lines=3, placeholder=\"Enter your conclusion...\")\n",
        "        result_output = gr.Markdown()\n",
        "\n",
        "        get_premise_btn = gr.Button(\"Load Premise\")\n",
        "        check_btn = gr.Button(\"Check Reasoning\")\n",
        "\n",
        "        get_premise_btn.click(populate_premise, inputs=topic_dropdown, outputs=premise_input)\n",
        "        check_btn.click(dnli_reasoning_check, inputs=[premise_input, hypothesis_input], outputs=result_output)\n",
        "\n",
        "app.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "id": "X035CnbpG-tp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}